# RL Learning Webapp - Product Specification

## ğŸ¯ Implementation Status (Dec 12, 2024)

**âœ… Completed (40% of MVP)**
- Next.js app with TypeScript, Tailwind CSS v3, and shadcn/ui
- Dashboard with progress tracking and curriculum overview
- Theory lesson viewer with markdown/LaTeX support
- Interactive quiz component with instant feedback
- Progress persistence using LocalStorage
- Dark/light mode theme system
- Basic routing and navigation

**ğŸš§ In Progress**
- Monaco Editor integration for code exercises
- Pyodide setup for Python execution in browser
- Interactive playground component

**ğŸ“‹ Next Priority**
- Complete code exercise component with test runner
- Add more curriculum content (currently only Module 1 has content)
- Implement achievements system

**See `implementation_status.md` for detailed progress report**

---

## Product Overview

An interactive web application that teaches reinforcement learning through a structured 16-week curriculum. The platform combines theory, quizzes, and hands-on coding exercises with an embedded code execution environment, allowing learners to write and run RL algorithms directly in the browser.

### Core Value Proposition
- **Learn by doing**: Tight feedback loop between theory and implementation
- **Self-paced**: Progress through curriculum at your own speed
- **Immediate feedback**: Run code and see results instantly
- **Visual learning**: Watch RL agents learn in real-time through interactive visualizations
- **Progress tracking**: Clear sense of achievement and momentum

### Target User
Self-directed learners with Python proficiency but limited ML/AI experience who want to master reinforcement learning through practical implementation.

---

## Technical Architecture Overview

### Frontend Stack
- **Framework**: Next.js (React)
- **Code Editor**: Monaco Editor (VS Code engine)
- **Python Runtime**: Pyodide (WebAssembly) for Weeks 1-4, server execution for Weeks 5+
- **Styling**: Tailwind CSS + Shadcn
- **Visualization**: Recharts, D3.js for custom RL visualizations - if complex, example images can be sourced offline
- **State Management**: React Context / Zustand

### Backend Stack (Phase 2)
- **API**: FastAPI (Python)
- **Database**: PostgreSQL (user progress, quiz results) - for localhost, use port 5438
- **Code Execution**: Docker containers for sandboxed Python execution
- **Authentication**: NextAuth.js

### Data Storage
- **Curriculum Content**: Static JSON/Markdown files
- **User Progress**: Database (lessons completed, quiz scores, exercise status)
- **User Code**: LocalStorage (MVP) â†’ Database (Phase 2)

---

## Screen-by-Screen Specifications

## 1. Dashboard / Home Screen

### Purpose
Central hub for navigation, progress tracking, and motivation.

### Layout
```
+----------------------------------------------------------+
|  [Logo]  RL Mastery                    [Profile] [Settings] |
+----------------------------------------------------------+
|                                                          |
|  Welcome back, Ankit! ğŸ‘‹                                |
|  You're currently in Week 2, Lesson 3                   |
|                                                          |
|  [Continue Learning â†’]                                   |
|                                                          |
|  +--------------------+  +---------------------------+  |
|  | Your Progress      |  | This Week                 |  |
|  |                    |  | â–¡ Lesson 1: Theory       |  |
|  | 18/64 Lessons âœ“    |  | âœ“ Lesson 2: Quiz         |  |
|  | 12/48 Exercises âœ“  |  | âŠ™ Lesson 3: Exercise     |  |
|  | Week 2 of 16       |  | â–¡ Lesson 4: Playground   |  |
|  +--------------------+  +---------------------------+  |
|                                                          |
|  Curriculum Overview                                    |
|  +------------------------------------------------+    |
|  | Week 1: RL Foundations        [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100%  |    |
|  | Week 2: Tabular Methods       [â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘]  50%  |    |
|  | Week 3: Function Approx       [â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘]   0%  |    |
|  | ...                                             |    |
|  +------------------------------------------------+    |
|                                                          |
|  Achievements                                           |
|  ğŸ¯ First Exercise    ğŸ”¥ 3-Day Streak    ğŸ“ˆ Quick Learner |
+----------------------------------------------------------+
```

### Components
1. **Hero Section**
   - Personalized greeting
   - Current location in curriculum
   - Primary CTA: "Continue Learning" button

2. **Progress Cards**
   - Overall stats (lessons, exercises, quizzes completed)
   - Current week breakdown with checkboxes
   - Visual progress bars

3. **Curriculum Map**
   - All 16 weeks listed
   - Status indicators: locked ğŸ”’, in progress âŠ™, completed âœ“
   - Click to navigate to any unlocked week
   - Progress bar per week

4. **Achievements/Gamification**
   - Streak counter
   - Badges for milestones
   - Learning stats (total hours, best streak)

### User Actions
- Click "Continue Learning" â†’ Navigate to next incomplete lesson
- Click any unlocked week â†’ Navigate to week overview
- Click Settings â†’ Go to settings screen
- Click Profile â†’ View/edit profile

### State Requirements
- User progress data (completed lessons, current position)
- Streak information
- Achievement unlocks
- Last active timestamp

---

## 2. Week Overview Screen

### Purpose
Preview week content and navigate between lessons.

### Layout
```
+----------------------------------------------------------+
|  â† Back to Dashboard                                     |
+----------------------------------------------------------+
|                                                          |
|  Week 2: Tabular Methods Deep Dive                      |
|                                                          |
|  This week you'll master the core tabular RL algorithms |
|  including SARSA, Q-learning, and Expected SARSA.       |
|                                                          |
|  Learning Objectives:                                   |
|  â€¢ Understand exploration vs exploitation               |
|  â€¢ Compare on-policy and off-policy methods             |
|  â€¢ Implement and debug tabular agents                   |
|                                                          |
|  +--------------------------------------------------+   |
|  | Lesson 1: Monte Carlo Methods        âœ“  [Review] |   |
|  | Lesson 2: Temporal Difference Learning âœ“ [Review] |   |
|  | Lesson 3: SARSA Algorithm            âŠ™  [Start]  |   |
|  | Lesson 4: Q-Learning vs SARSA        ğŸ”’ [Locked]  |   |
|  | Lesson 5: Week 2 Playground          ğŸ”’ [Locked]  |   |
|  | Week 2 Checkpoint Quiz               ğŸ”’ [Locked]  |   |
|  +--------------------------------------------------+   |
|                                                          |
|  Estimated time: 8-10 hours                             |
|  Prerequisites: Week 1 completed âœ“                      |
|                                                          |
+----------------------------------------------------------+
```

### Components
1. **Week Header**
   - Week number and title
   - Brief description
   - Learning objectives (bulleted list)

2. **Lesson List**
   - All lessons in order
   - Status icons (completed, in progress, locked)
   - Primary action button (Review/Start/Continue/Locked)
   - Lesson type indicator (Theory/Quiz/Exercise/Playground)

3. **Week Metadata**
   - Estimated completion time
   - Prerequisites
   - Optional: Resources (papers, videos)

### User Actions
- Click lesson with "Start/Continue" â†’ Go to that lesson
- Click "Review" â†’ Re-enter completed lesson
- Navigate back to dashboard
- Locked lessons show tooltip explaining unlock requirements

### State Requirements
- Per-lesson completion status
- Current active lesson
- Lock/unlock logic based on prerequisite completion

---

## 3. Lesson View Screen (Theory)

### Purpose
Primary learning interface for reading and understanding concepts.

### Layout
```
+----------------------------------------------------------+
|  Week 2 > Lesson 3: SARSA Algorithm                      |
+----------------------------------------------------------+
| [TOC â–¼]            |  MAIN CONTENT AREA                  |
|                    |                                      |
| Introduction       |  # SARSA: On-Policy TD Control      |
| â€¢ What is SARSA?   |                                      |
| â€¢ Key Differences  |  SARSA (State-Action-Reward-State-  |
| The Algorithm      |  Action) is an on-policy temporal   |
| â€¢ Update Rule      |  difference control algorithm...    |
| â€¢ Pseudocode       |                                      |
| Example            |  ## The Update Rule                 |
| â€¢ GridWorld        |                                      |
| Comparison         |  Q(S,A) â† Q(S,A) + Î±[R + Î³Q(S',A')  |
| â€¢ SARSA vs Q-L     |                - Q(S,A)]             |
| Key Takeaways      |                                      |
|                    |  [Diagram: SARSA update cycle]      |
|                    |                                      |
|                    |  Unlike Q-learning, SARSA updates   |
|                    |  based on the action actually taken |
|                    |  by the current policy...           |
|                    |                                      |
|                    |  ## Example: GridWorld              |
|                    |                                      |
|                    |  [Interactive visualization]        |
|                    |                                      |
|                    |  +------------------------------+   |
|                    |  | ğŸ’¡ Key Insight                |   |
|                    |  | SARSA learns the value of the |   |
|                    |  | policy it's following, making |   |
|                    |  | it more conservative than     |   |
|                    |  | Q-learning in risky envs.     |   |
|                    |  +------------------------------+   |
|                    |                                      |
+--------------------+--------------------------------------+
| â† Previous Section                   Next: Quiz â†’ |
+----------------------------------------------------------+
| Reading Progress: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘  60%                    |
+----------------------------------------------------------+
```

### Components
1. **Navigation Sidebar (Left, Sticky)**
   - Table of contents with nested sections
   - Click to jump to section
   - Current section highlighted
   - Collapsible/expandable

2. **Content Area**
   - Markdown-rendered theory
   - LaTeX math rendering (for equations)
   - Syntax-highlighted code snippets (read-only examples)
   - Embedded images/diagrams
   - Callout boxes (Tips, Warnings, Key Insights)
   - Interactive visualizations where applicable

3. **Navigation Footer**
   - Previous/Next section buttons
   - Progress bar showing reading completion
   - Breadcrumb trail

### Interactions
- Scroll to read, auto-saves reading position
- Click TOC items to jump to sections
- Click code snippets to copy to clipboard
- Hover over terms for definitions (tooltip)
- Optional: Highlight text to save notes

### Content Types
- **Text**: Explanations, examples
- **Equations**: LaTeX-rendered formulas
- **Code**: Syntax-highlighted Python examples
- **Diagrams**: Static images or interactive SVG
- **Callouts**: Important concepts, warnings, pro tips
- **Videos**: Embedded short explanations (optional)

---

## 4. Quiz Screen

### Purpose
Test understanding of theory concepts with immediate feedback.

### Layout - Question View
```
+----------------------------------------------------------+
|  Week 2 > Lesson 3 Quiz                                  |
+----------------------------------------------------------+
|                                                          |
|  Question 3 of 7                           [â–‘â–‘â–‘â–ˆâ–‘â–‘â–‘]    |
|                                                          |
|  Which statement is TRUE about SARSA?                   |
|                                                          |
|  â—‹ SARSA is an off-policy algorithm                     |
|  â—‹ SARSA updates Q-values based on the greedy action    |
|  â— SARSA learns the value of the policy being followed  |
|  â—‹ SARSA always converges faster than Q-learning        |
|                                                          |
|                                    [Show Hint] [Submit]  |
|                                                          |
+----------------------------------------------------------+
```

### Layout - After Submission
```
+----------------------------------------------------------+
|  Week 2 > Lesson 3 Quiz                                  |
+----------------------------------------------------------+
|                                                          |
|  Question 3 of 7                           [â–‘â–‘â–‘â–ˆâ–‘â–‘â–‘]    |
|                                                          |
|  Which statement is TRUE about SARSA?                   |
|                                                          |
|  â—‹ SARSA is an off-policy algorithm                     |
|  â—‹ SARSA updates Q-values based on the greedy action    |
|  âœ“ SARSA learns the value of the policy being followed  |
|  â—‹ SARSA always converges faster than Q-learning        |
|                                                          |
|  +------------------------------------------------------+|
|  | âœ“ Correct!                                           ||
|  |                                                      ||
|  | SARSA is an on-policy algorithm, meaning it learns  ||
|  | the value of the policy it's currently following.   ||
|  | This is why it's called State-Action-Reward-State-  ||
|  | Action - it uses the actual next action A'.         ||
|  +------------------------------------------------------+|
|                                                          |
|                                            [Next â†’]      |
|                                                          |
+----------------------------------------------------------+
```

### Layout - End Summary
```
+----------------------------------------------------------+
|  Quiz Complete! ğŸ‰                                       |
+----------------------------------------------------------+
|                                                          |
|  Your Score: 6/7 (86%)                                  |
|                                                          |
|  +------------------------------------------------------+|
|  | âœ“ Question 1: What is SARSA?                         ||
|  | âœ“ Question 2: On-policy vs off-policy                ||
|  | âœ“ Question 3: SARSA characteristics                  ||
|  | âœ— Question 4: Convergence guarantees                 ||
|  | âœ“ Question 5: Update rule                            ||
|  | âœ“ Question 6: Exploration strategies                 ||
|  | âœ“ Question 7: Comparison with Q-learning             ||
|  +------------------------------------------------------+|
|                                                          |
|  Topics to Review:                                      |
|  â€¢ Convergence conditions for tabular methods           |
|                                                          |
|  [Review Incorrect] [Retake Quiz] [Continue to Exercise]|
|                                                          |
+----------------------------------------------------------+
```

### Components
1. **Question Display**
   - Question number and total
   - Progress indicator
   - Question text (may include code snippets or diagrams)
   - Answer options (radio buttons for single choice, checkboxes for multiple)

2. **Hint System**
   - "Show Hint" button
   - Progressive hints (Hint 1 is gentle, Hint 2 more direct)
   - Hints don't reveal answer directly

3. **Feedback Panel** (appears after submission)
   - Correct/Incorrect indicator with color coding
   - Explanation of why answer is right/wrong
   - Reference to relevant lesson section

4. **Summary Screen**
   - Overall score
   - Question-by-question breakdown
   - Topics to review (based on incorrect answers)
   - Actions: Review, Retake, or Continue

### Question Types
- Multiple choice (single answer)
- Multiple select (multiple correct answers)
- True/False
- Fill-in-the-blank (text input)
- Code output prediction ("What does this code print?")

### User Actions
- Select answer â†’ Enable Submit button
- Click Submit â†’ Show feedback, advance to next
- Click Hint â†’ Reveal progressive hint (optional)
- Click "Review Incorrect" â†’ Jump to relevant lesson sections
- Click "Retake Quiz" â†’ Reset and start over
- Click "Continue" â†’ Proceed to next lesson

### State Requirements
- Current question index
- User's answer selections
- Correct/incorrect status per question
- Hints used count
- Final score
- Timestamp of completion

---

## 5. Coding Exercise Screen

### Purpose
The core interactive experience where users implement RL algorithms and see them run.

### Layout
```
+----------------------------------------------------------+
|  Week 2 > Lesson 3: Implement SARSA                      |
+----------------------------------------------------------+
|  INSTRUCTIONS          |  CODE EDITOR                    |
|  (scrollable)          |                                 |
+------------------------+                                 |
|                        |  1  import numpy as np          |
| # Exercise: SARSA      |  2  from collections...         |
|                        |  3                               |
| Implement the SARSA    |  4  def sarsa(env, episodes...  |
| algorithm for the      |  5      # TODO: Initialize...  |
| FrozenLake environment.|  6      Q = np.zeros((env...   |
|                        |  7                               |
| ## Your Task           |  8      for episode in...      |
| Complete the sarsa()   |  9          state = env.reset() |
| function below.        | 10          # TODO: Choose...  |
|                        | 11          action = ...        |
| ## Requirements        | 12                               |
| 1. Initialize Q-table  |                                 |
| 2. Select actions with |                                 |
| epsilon-greedy         |                                 |
| 3. Update Q-values     +------------------------+--------+
| using SARSA rule       | [Run Code] [Run Tests] [Reset] |
| 4. Track returns       +--------------------------------+
|                        |                                 |
| ## Starter Code        | OUTPUT PANEL                    |
| We've provided...      | [Console] [Plots] [Tests]      |
|                        +--------------------------------+
| ## Test Cases          |                                 |
| Your implementation    | >>> Running your code...        |
| will be tested with... | Episode 100: Avg Return = -0.52 |
|                        | Episode 200: Avg Return = -0.31 |
| [Show Hints â–¼]         | Episode 500: Avg Return =  0.12 |
|                        | Training complete!              |
+------------------------+---------------------------------+
```

### Layout - With Visualization
```
+----------------------------------------------------------+
|  OUTPUT PANEL                                            |
+----------------------------------------------------------+
| [Console] [Plots] [Tests] [Visualization]               |
+----------------------------------------------------------+
|                                                          |
|  Learning Curve                                         |
|  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            |
|  â”‚                              ___---      â”‚            |
|  â”‚                       __----              â”‚            |
|  â”‚             ___----                       â”‚            |
|  â”‚   ___----                                 â”‚            |
|  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            |
|    0      100     200     300     400    500            |
|                   Episodes                              |
|                                                          |
|  Q-Value Heatmap (Final Policy)                        |
|  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                    |
|  â”‚ S  â†’  â†’  â†’  G  â”‚  (S = Start, G = Goal)             |
|  â”‚ â†“  H  â†’  H  â†“  â”‚  (H = Hole)                        |
|  â”‚ â†’  â†’  â†’  H  â†“  â”‚  (Arrows = best action)            |
|  â”‚ H  â†’  â†’  â†’  G  â”‚                                     |
|  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                    |
|                                                          |
+----------------------------------------------------------+
```

### Layout - Test Results
```
+----------------------------------------------------------+
| [Console] [Plots] [Tests] [Visualization]               |
+----------------------------------------------------------+
|                                                          |
|  Test Results                                           |
|                                                          |
|  âœ“ Test 1: Q-table initialized correctly                |
|  âœ“ Test 2: Epsilon-greedy action selection works        |
|  âœ“ Test 3: SARSA update rule implemented correctly      |
|  âœ— Test 4: Agent learns to reach goal                   |
|                                                          |
|  Error in Test 4:                                       |
|  Expected: Average return > 0.5 after 1000 episodes     |
|  Got: Average return = 0.23                             |
|                                                          |
|  Hint: Try increasing epsilon to explore more, or       |
|  adjusting the learning rate.                           |
|                                                          |
|  [View Full Test Code]                                  |
|                                                          |
+----------------------------------------------------------+
```

### Components

**1. Instructions Panel (Left, Scrollable)**
- Exercise description
- Task breakdown with numbered requirements
- Starter code explanation
- Test case descriptions
- Hints (collapsible sections)
- Expected outputs/behavior

**2. Code Editor (Right Top)**
- Monaco Editor with Python syntax highlighting
- Line numbers
- Basic autocomplete
- Error underlining
- Pre-filled starter code with TODO comments
- Keyboard shortcuts (Cmd/Ctrl+Enter to run)

**3. Control Buttons**
- **Run Code**: Execute code, show console output and plots
- **Run Tests**: Execute test suite, show pass/fail
- **Reset**: Restore to starter code (with confirmation)
- **Show Solution**: Unlock after X attempts or Y minutes (shows reference implementation)

**4. Output Panel (Right Bottom, Tabbed)**

**Tab: Console**
- stdout/stderr from code execution
- Print statements
- Error messages with line numbers
- Execution time

**Tab: Plots**
- Matplotlib visualizations rendered as images
- Learning curves (rewards over episodes)
- Q-value heatmaps
- Policy visualizations
- Agent trajectory animations (for gridworld-type envs)

**Tab: Tests**
- List of test cases with âœ“/âœ— status
- Failed test details (expected vs actual)
- Hints for failing tests
- Code coverage (advanced feature)

**Tab: Visualization** (Interactive)
- Live agent behavior in environment
- Step-through controls (play/pause/step)
- Speed controls
- Episode selector
- State/action/reward display

**5. Status Bar**
- Execution status (Ready / Running... / Success / Error)
- "Mark as Complete" button (appears when tests pass)
- Time spent on exercise

**6. Hints System** (Collapsible in instructions)
- Progressive hints (not all visible at once)
- "Show Hint 1" â†’ reveals first hint
- "Show Hint 2" â†’ reveals more direct guidance
- Each hint has "Was this helpful?" feedback

### Code Execution Flow
1. User writes code in editor
2. Clicks "Run Code"
3. Code sent to execution engine (Pyodide or backend)
4. Execution runs with timeout (30 seconds default)
5. Results returned:
   - Console output (stdout/stderr)
   - Generated plots (base64 images)
   - Variable values (if requested)
   - Execution time
6. Results displayed in Output Panel
7. Auto-scroll to show results

### Test Execution Flow
1. User clicks "Run Tests"
2. User code combined with test harness
3. Tests executed in isolated environment
4. Each test case returns: pass/fail, actual vs expected, error message
5. Summary displayed: X/Y tests passed
6. If all pass: "Mark as Complete" button enabled + confetti animation ğŸ‰

### User Actions
- Write/edit code
- Run code to see outputs
- Run tests to verify correctness
- View hints when stuck
- Reset to starter code if needed
- View solution (after attempts)
- Mark as complete â†’ Unlock next lesson
- Save code (auto-save every 30 seconds)

### State Requirements
- User's current code (persisted)
- Test results (current session)
- Hints revealed count
- Attempts count
- Time spent
- Completion status
- Saved solution snapshots (optional: user can save working versions)

---

## 6. Interactive Playground Screen

### Purpose
Experiment with RL algorithms and hyperparameters to build intuition.

### Layout
```
+----------------------------------------------------------+
|  Week 2 > SARSA Playground                               |
+----------------------------------------------------------+
|                                                          |
|  CONTROLS             |  VISUALIZATION                   |
+------------------------+                                 |
| Environment           |   FrozenLake 4x4                 |
| [FrozenLake 4x4 â–¼]    |                                 |
|                       |   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           |
| Algorithm             |   â”‚ S  F  F  F     â”‚           |
| [SARSA â–¼]             |   â”‚ F  H  F  H     â”‚           |
|                       |   â”‚ F  F  F  H     â”‚           |
| Learning Rate (Î±)     |   â”‚ H  F  F  G  ğŸ¤– â”‚           |
| 0.1 [â”€â”€â”€â”€â”€â—â”€â”€â”€â”€â”€] 0.5 |   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           |
|                       |                                 |
| Discount (Î³)          |   Episode: 247 / 500            |
| 0.9 [â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—] 1.0 |   Current Reward: +1.0          |
|                       |   Total Return: 0.42            |
| Epsilon (Îµ)           |                                 |
| 0.0 [â”€â”€â”€â”€â—â”€â”€â”€â”€â”€â”€] 1.0 |   [â–¶ Play] [â¸ Pause] [â­ Step]  |
|                       |   Speed: [â”€â”€â—â”€â”€â”€â”€â”€â”€â”€â”€]          |
| Episodes              +----------------------------------+
| 500                   |                                 |
|                       |  METRICS                        |
| [â–¶ Run Simulation]    |  [Learning Curve] [Q-Values]   |
| [Reset]               +----------------------------------+
|                       |                                 |
| [+ Add to Compare]    |  Average Return                 |
|                       |   1.0 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€       |
|                       |       â”‚          ___---         |
| Comparison (2/3)      |   0.5 â”‚    __---                |
| â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  |       â”‚__--                     |
| â”‚Config 1: Î±=0.1   â”‚  |   0.0 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€       |
| â”‚  Return: 0.42    â”‚  |       0    100   200   500      |
| â”‚Config 2: Î±=0.3   â”‚  |                                 |
| â”‚  Return: 0.58    â”‚  |  [Download Data] [Save Config]  |
| â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  |                                 |
+-----------------------+----------------------------------+
```

### Components

**1. Control Panel (Left)**

**Environment Selector**
- Dropdown: FrozenLake-v1 (4x4), FrozenLake-v1 (8x8), CliffWalking, GridWorld
- Environment renders in visualization area

**Algorithm Selector**
- Dropdown: SARSA, Q-Learning, Expected SARSA, Monte Carlo
- Algorithm-specific parameters appear dynamically

**Hyperparameter Sliders**
- Learning rate (Î±): 0.01 to 0.5
- Discount factor (Î³): 0.9 to 1.0
- Epsilon (Îµ): 0.0 to 1.0 (for epsilon-greedy)
- Episodes: 100 to 2000
- Real-time value display next to slider

**Simulation Controls**
- "Run Simulation" button (primary action)
- "Reset" button (clears current run)
- "Add to Comparison" (saves current config to comparison panel)

**Comparison Panel**
- Shows up to 3 saved configurations
- Each shows: algorithm, key params, final performance
- Click to highlight in metrics area
- "Clear All" button

**2. Visualization Area (Center Top)**

**Environment Display**
- Grid-based environments rendered visually
- Agent position shown with emoji/icon
- Current state highlighted
- Goal/holes/obstacles marked
- Real-time updates as agent moves

**Playback Controls**
- Play/Pause button
- Step forward (single step)
- Speed slider (1x to 10x)
- Episode counter
- Current reward display

**3. Metrics Area (Center Bottom)**

**Tabs: Learning Curve / Q-Values / Policy / Comparison**

**Learning Curve Tab**
- Line chart: Average return over episodes
- Smoothed with moving average
- X-axis: Episodes, Y-axis: Return
- Multiple lines if comparing configs

**Q-Values Tab**
- Heatmap showing Q-values for each state-action pair
- Color intensity = value magnitude
- Can select specific episode to view
- Scrubber to see evolution over training

**Policy Tab**
- Visual representation of learned policy
- Arrows showing best action per state
- Grid overlay with action directions
- Value function heatmap overlay (optional)

**Comparison Tab** (when multiple configs saved)
- Side-by-side or overlay comparison
- All configs on same chart
- Legend with config details
- Statistical comparison (convergence speed, final performance)

**4. Export/Save**
- Download learning data (CSV)
- Save configuration (JSON)
- Share link (encodes parameters in URL)
- "Copy to Exercise" button (transfers setup to coding exercise)

### Interactions
1. **Adjust parameters** â†’ Sliders update in real-time
2. **Click "Run Simulation"** â†’ 
   - Starts training
   - Visualization animates agent
   - Metrics update live
   - Can pause/resume
3. **Click "Add to Comparison"** â†’
   - Current config saved
   - Appears in comparison panel
   - Metrics show overlay
4. **Select environment** â†’ Visualization updates to show new env
5. **Hover over states** â†’ Tooltip shows Q-values for that state

### Preset Scenarios (Optional)
- "Cautious Learner" (low Îµ, low Î±)
- "Bold Explorer" (high Îµ, high Î±)
- "Quick Adapter" (high Î±, decay Îµ)
- "Patient Optimizer" (many episodes, low Î±)
- Helps users understand hyperparameter effects

### State Requirements
- Current parameter values
- Simulation state (running/paused/completed)
- Comparison configs (up to 3)
- Learning data (for plotting)
- Current episode/step

---

## 7. Week Checkpoint Screen

### Purpose
Comprehensive assessment and review before advancing to next week.

### Layout
```
+----------------------------------------------------------+
|  Week 2 Checkpoint: Tabular Methods Mastery             |
+----------------------------------------------------------+
|                                                          |
|  You've completed all lessons in Week 2! ğŸ‰             |
|  Let's verify you're ready for Week 3.                  |
|                                                          |
|  Your Week 2 Progress                                   |
|  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               |
|  â”‚ âœ“ 4/4 Theory Lessons                 â”‚               |
|  â”‚ âœ“ 4/4 Quizzes (avg: 88%)             â”‚               |
|  â”‚ âœ“ 3/3 Coding Exercises               â”‚               |
|  â”‚ âœ“ 1/1 Playground Exploration         â”‚               |
|  â”‚ âŠ™ 0/1 Checkpoint Quiz                â”‚               |
|  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               |
|                                                          |
|  Key Concepts This Week                                 |
|  â€¢ Monte Carlo methods for episodic tasks               |
|  â€¢ Temporal Difference learning (TD)                    |
|  â€¢ SARSA (on-policy TD control)                         |
|  â€¢ Q-learning (off-policy TD control)                   |
|  â€¢ Exploration vs exploitation strategies               |
|                                                          |
|  Checkpoint Quiz                                        |
|  Take this 15-question quiz to test your mastery        |
|  of Week 2 concepts. You need 70% to unlock Week 3.     |
|                                                          |
|  [Start Checkpoint Quiz]                                |
|                                                          |
|  Not ready yet?                                         |
|  [Review Week 2 Lessons]                                |
|                                                          |
+----------------------------------------------------------+
```

### After Quiz - Pass Scenario
```
+----------------------------------------------------------+
|  Checkpoint Complete! âœ…                                 |
+----------------------------------------------------------+
|                                                          |
|  Excellent work! You scored 13/15 (87%)                 |
|                                                          |
|  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   |
|  â”‚ Strong Areas:                                    â”‚   |
|  â”‚ âœ“ Monte Carlo methods                            â”‚   |
|  â”‚ âœ“ TD learning basics                             â”‚   |
|  â”‚ âœ“ SARSA implementation                           â”‚   |
|  â”‚                                                  â”‚   |
|  â”‚ Topics to Review (optional):                     â”‚   |
|  â”‚ â€¢ Convergence guarantees                         â”‚   |
|  â”‚ â€¢ Temporal difference error                      â”‚   |
|  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   |
|                                                          |
|  ğŸ¯ Achievement Unlocked: Tabular Methods Master        |
|                                                          |
|  You're ready for Week 3: Function Approximation!       |
|                                                          |
|  [Proceed to Week 3 â†’]                                  |
|  [Review Missed Topics]                                 |
|                                                          |
+----------------------------------------------------------+
```

### After Quiz - Needs Review
```
+----------------------------------------------------------+
|  Almost There! ğŸ“š                                        |
+----------------------------------------------------------+
|                                                          |
|  You scored 9/15 (60%)                                  |
|  You need 70% to proceed to Week 3.                     |
|                                                          |
|  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   |
|  â”‚ Topics to Review:                                â”‚   |
|  â”‚ â€¢ SARSA vs Q-learning differences                â”‚   |
|  â”‚ â€¢ TD error and update rules                      â”‚   |
|  â”‚ â€¢ Exploration strategies                         â”‚   |
|  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   |
|                                                          |
|  Recommended Actions:                                   |
|  1. Review Lesson 3: SARSA Algorithm                    |
|  2. Review Lesson 4: Q-Learning vs SARSA                |
|  3. Re-attempt coding exercises with different params   |
|  4. Revisit the Playground to build intuition           |
|                                                          |
|  [Review Recommended Lessons]                           |
|  [Retake Checkpoint Quiz]                               |
|                                                          |
+----------------------------------------------------------+
```

### Components
1. **Progress Summary**
   - Checklist of all week components
   - Completion percentages
   - Time spent this week

2. **Key Concepts Recap**
   - Bulleted list of main topics covered
   - Quick reference for what you should know

3. **Checkpoint Quiz**
   - 10-15 comprehensive questions
   - Covers all week topics
   - Pass threshold: 70%
   - Can retake if needed

4. **Results Screen**
   - Score and pass/fail status
   - Breakdown by topic area
   - Strengths and weaknesses identified
   - Recommendations for review

5. **Unlock Mechanism**
   - Pass checkpoint â†’ Week 3 unlocked + achievement badge
   - Fail checkpoint â†’ Recommendations + retake option
   - Can review lessons without retaking quiz

### User Actions
- Start checkpoint quiz
- Review lessons (if not ready)
- Retake quiz (if failed)
- Proceed to next week (if passed)
- Download week summary (PDF/Markdown)

---

## 8. Reference / Cheatsheet Screen

### Purpose
Quick lookup for formulas, algorithms, and common patterns.

### Layout
```
+----------------------------------------------------------+
|  RL Reference & Cheatsheet                    [SearchğŸ”] |
+----------------------------------------------------------+
| CATEGORIES              |  CONTENT                       |
|                         |                                |
| Foundations             |  # Bellman Equation            |
| â€¢ Bellman Equations     |                                |
| â€¢ Value Functions       |  The Bellman equation expresses|
| â€¢ Policies              |  the recursive relationship... |
|                         |                                |
| Algorithms              |  V(s) = Î£â‚ Ï€(a|s) Î£â‚›',áµ£ p(...)|
| â–¸ Tabular Methods       |                                |
|   â€¢ Monte Carlo         |  [Copy Equation] [View Lesson] |
|   â€¢ TD Learning         |                                |
|   â€¢ SARSA               |  ## Implementation             |
|   â€¢ Q-Learning          |  ```python                     |
| â–¸ Function Approx       |  def bellman_update(V, s, ...  |
| â–¸ Policy Gradients      |  ```                           |
|                         |  [Copy Code]                   |
| Code Templates          |                                |
| â€¢ Q-Table Init          |                                |
| â€¢ Epsilon-Greedy        |  ## When to Use                |
| â€¢ Training Loop         |  - For computing exact value...|
|                         |  - As basis for all RL alg...  |
| Common Patterns         |                                |
| â€¢ Reward Shaping        +--------------------------------+
| â€¢ Exploration Decay     |                                |
| â€¢ Hyperparameter Tips   |                                |
|                         |                                |
| Glossary                |                                |
| â€¢ State (S)             |                                |
| â€¢ Action (A)            |                                |
| â€¢ Reward (R)            |                                |
| â€¢ Policy (Ï€)            |                                |
+-------------------------+--------------------------------+
```

### Components
1. **Category Sidebar**
   - Collapsible sections
   - Quick navigation
   - Current selection highlighted

2. **Search Bar**
   - Full-text search across all content
   - Filters: All / Formulas / Code / Concepts
   - Recent searches

3. **Content Display**
   - Rendered markdown
   - Syntax-highlighted code
   - LaTeX equations
   - Copy buttons for code/equations
   - "View in Lesson" links back to source

### Content Categories

**Foundations**
- MDP definition
- Bellman equations (expectation & optimality)
- Value functions (V, Q)
- Policy types
- Discount factor intuition

**Algorithms Comparison Table**
| Algorithm | On/Off-Policy | Update Rule | Best For |
|-----------|---------------|-------------|----------|
| Monte Carlo | On | Full episode | Episodic tasks |
| SARSA | On | TD(0) | Safe learning |
| Q-Learning | Off | TD(0) | Optimal policy |
| ... | | | |

**Code Templates**
- Q-table initialization
- Epsilon-greedy selection
- Standard training loop
- Plotting learning curves
- Environment wrappers

**Common Debugging Tips**
- Agent not learning â†’ Check learning rate, exploration
- Divergence â†’ Reduce LR, check reward scale
- Slow convergence â†’ Increase episodes, tune hyperparams

**Glossary**
- Alphabetical list of RL terms
- Short definitions with examples
- Links to detailed explanations

### User Actions
- Search for terms
- Navigate categories
- Copy code/equations
- Click "View in Lesson" to review
- Bookmark frequently referenced items

---

## 9. Settings / Profile Screen

### Layout
```
+----------------------------------------------------------+
|  Settings                                                |
+----------------------------------------------------------+
|                                                          |
|  Profile                                                |
|  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   |
|  â”‚ Name: Ankit                               [Edit] â”‚   |
|  â”‚ Email: ankit@example.com                  [Edit] â”‚   |
|  â”‚ Started: Dec 1, 2024                             â”‚   |
|  â”‚ Total Learning Time: 23.5 hours                  â”‚   |
|  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   |
|                                                          |
|  Learning Preferences                                   |
|  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   |
|  â”‚ Code Editor Theme                                â”‚   |
|  â”‚ [Dark â–¼]                                         â”‚   |
|  â”‚                                                  â”‚   |
|  â”‚ Font Size                                        â”‚   |
|  â”‚ [14px â–¼]                                         â”‚   |
|  â”‚                                                  â”‚   |
|  â”‚ Auto-Save Code                                   â”‚   |
|  â”‚ [âœ“] Every 30 seconds                             â”‚   |
|  â”‚                                                  â”‚   |
|  â”‚ Show Hints by Default                            â”‚   |
|  â”‚ [ ] Always visible (not recommended)             â”‚   |
|  â”‚                                                  â”‚   |
|  â”‚ Keyboard Shortcuts                               â”‚   |
|  â”‚ [âœ“] Enable (Cmd+Enter to run, etc.)              â”‚   |
|  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   |
|                                                          |
|  Goals & Notifications                                  |
|  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   |
|  â”‚ Weekly Learning Goal                             â”‚   |
|  â”‚ [10 hours â–¼]                                     â”‚   |
|  â”‚                                                  â”‚   |
|  â”‚ Target Completion Date                           â”‚   |
|  â”‚ [March 15, 2025]      (16 weeks from start)      â”‚   |
|  â”‚                                                  â”‚   |
|  â”‚ Email Reminders                                  â”‚   |
|  â”‚ [âœ“] Daily reminder (9:00 AM)                     â”‚   |
|  â”‚ [âœ“] Weekly summary (Sundays)                     â”‚   |
|  â”‚ [ ] Inactivity nudge (after 3 days)              â”‚   |
|  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   |
|                                                          |
|  Data & Privacy                                         |
|  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   |
|  â”‚ [Export My Data]                                 â”‚   |
|  â”‚ Download all your code, progress, and quiz       â”‚   |
|  â”‚ results as a ZIP file.                           â”‚   |
|  â”‚                                                  â”‚   |
|  â”‚ [Reset Progress]                                 â”‚   |
|  â”‚ Clear all progress and start fresh.              â”‚   |
|  â”‚ âš ï¸ This cannot be undone.                        â”‚   |
|  â”‚                                                  â”‚   |
|  â”‚ [Delete Account]                                 â”‚   |
|  â”‚ Permanently delete your account and all data.    â”‚   |
|  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   |
|                                                          |
+----------------------------------------------------------+
```

---

## Critical User Flows

### Flow 1: First-Time User Onboarding

**Steps:**
1. User lands on app â†’ No auth required for MVP (local storage)
2. Welcome modal appears:
   - "Welcome to RL Mastery! ğŸ‘‹"
   - "This 16-week curriculum will teach you reinforcement learning through hands-on coding."
   - "Ready to start?"
   - [Get Started] button

3. Brief tour (optional skip):
   - Highlight: "This is your dashboard"
   - Highlight: "Each week has theory, quizzes, and coding"
   - Highlight: "Write and run code directly in browser"
   - [Skip Tour] / [Next â†’]

4. Redirect to Week 1, Lesson 1 (RL Foundations intro)

5. User reads theory â†’ Takes first quiz â†’ Completes first exercise

6. Success celebration:
   - "ğŸ‰ You completed your first exercise!"
   - "You learned: Multi-armed bandits"
   - [Continue to Next Lesson]

7. User navigates freely from here

**Key Success Metrics:**
- % who complete first exercise
- Time to first exercise completion
- Drop-off points in onboarding

---

### Flow 2: Daily Learning Session

**Steps:**
1. User returns to app â†’ Dashboard loads
2. Shows: "Welcome back! Resume Week 2, Lesson 3"
3. Click [Continue Learning] â†’ Jumps to Lesson 3 coding exercise
4. User's previous code already loaded (auto-saved)
5. User continues working, runs code, debugs
6. Tests pass â†’ Confetti animation ğŸ‰
7. [Mark as Complete] â†’ Exercise marked done
8. [Next Lesson] â†’ Loads Lesson 4 theory
9. User reads theory, realizes it's getting late
10. Closes browser â†’ Progress auto-saved
11. Returns next day â†’ Right where they left off

**Auto-save Points:**
- Code editor: every 30 seconds
- Reading position: on scroll stop
- Quiz answers: on selection
- All data in localStorage (Phase 1) or DB (Phase 2)

---

### Flow 3: Struggling with Concept

**Scenario:** User stuck on Q-learning exercise

**Steps:**
1. User writes code, clicks [Run Tests]
2. Tests fail: "Expected: Agent reaches goal 80% of time. Got: 20%"
3. User reads error, tries tweaking learning rate
4. Still failing after 3 attempts
5. Click [Show Hint 1]:
   - "Check your exploration rate. Are you exploring enough to find the goal?"
6. User adjusts epsilon, runs again
7. Still failing
8. Click [Show Hint 2]:
   - "Your Q-value update might not be using the max of the next state. Remember Q-learning is off-policy."
9. User reviews SARSA vs Q-learning lesson (link in hint)
10. "Aha!" moment â†’ Fixes code
11. Tests pass â†’ Relief and satisfaction
12. Proceeds to next lesson with confidence

**Support Mechanisms:**
- Progressive hints (don't give away answer)
- Links back to relevant theory
- Show solution after 30 minutes or 5 attempts (whichever first)
- "Was this helpful?" feedback on hints

---

### Flow 4: Building Intuition in Playground

**Steps:**
1. User finishes SARSA coding exercise
2. Sees "Explore SARSA Playground" button
3. Clicks â†’ Playground loads with SARSA on FrozenLake
4. Default parameters shown, clicks [Run Simulation]
5. Watches agent learn, sees learning curve
6. Notices agent converges around episode 300
7. Adjusts epsilon from 0.1 to 0.5 â†’ [Run Simulation]
8. Agent explores more, converges faster! (episode 200)
9. Clicks [Add to Comparison] â†’ Config saved
10. Tries epsilon = 0.01 (very low exploration)
11. Agent barely explores, never finds goal
12. Comparison shows all 3 configs side-by-side
13. "Aha! Exploration really matters"
14. Downloads learning data for future reference
15. Proceeds to next lesson with deeper understanding

**Learning Amplifiers:**
- Visual feedback (see agent move)
- Immediate results (no waiting)
- Easy experimentation (sliders)
- Comparison enables insight
- No penalty for "failing" (it's exploration!)

---

### Flow 5: Week Completion & Checkpoint

**Steps:**
1. User completes last lesson of Week 2
2. Dashboard shows: "Week 2 Complete! Take checkpoint quiz"
3. Click [Take Checkpoint] â†’ Checkpoint screen loads
4. Reviews key concepts, feels ready
5. Click [Start Checkpoint Quiz]
6. Answers 15 questions, scores 13/15 (87%)
7. Results screen:
   - "Excellent! You're ready for Week 3"
   - Shows strengths and minor weaknesses
   - Achievement badge unlocked
8. Week 3 unlocks on dashboard (was previously locked)
9. Celebration modal: "ğŸ‰ Week 2 Complete! ğŸš€"
10. [Proceed to Week 3] or [Take a Break]
11. User proceeds, sees Week 3 overview
12. Excited about function approximation and neural nets

**Motivation Boosters:**
- Clear pass/fail with encouraging language
- Achievement badges and unlocks
- Visual progress (14 more weeks to go!)
- Option to take break vs continue immediately

---

### Flow 6: Returning After Gap

**Scenario:** User takes 1-week break, returns

**Steps:**
1. User returns to dashboard
2. Message: "Welcome back! It's been 7 days. You left off in Week 3, Lesson 2."
3. Optional: Quick recap of Week 3 so far
4. Click [Resume Learning] â†’ Loads Lesson 2
5. Code editor still has their previous code (saved)
6. Feels familiar, continues where left off
7. Streak broken, but progress intact
8. Completes lesson, momentum rebuilds

**Re-engagement:**
- Auto-save prevents loss of work
- Gentle reminder, not guilt
- Quick recap helps refresh memory
- Progress intact = low friction to continue

---

## Mobile Considerations

### MVP: Desktop-First, Mobile-Friendly Reading

**What works on mobile:**
- âœ… Theory lessons (scrollable text)
- âœ… Quizzes (tap to select, submit)
- âœ… Dashboard & navigation
- âœ… Progress tracking
- âœ… Reference/cheatsheet

**What's challenging on mobile:**
- âŒ Code editor (typing on phone keyboard)
- âŒ Playground (complex interactions)
- âš ï¸ Visualizations (render but may be cramped)

### Recommended Approach
- **Phase 1 (MVP)**: Optimize for desktop/laptop, make theory/quizzes responsive for mobile reading
- **Phase 2**: Consider tablet-friendly code editor (larger screen, external keyboard)
- **Future**: Mobile app for theory/quizzes, require desktop for coding

### Responsive Breakpoints
- **Desktop** (â‰¥1024px): Full side-by-side layout
- **Tablet** (768-1023px): Stacked layout, code editor still usable
- **Mobile** (â‰¤767px): Reading mode only, suggest desktop for coding

---

## State Management & Data Persistence

### What Data Needs to Persist?

**User Profile**
- Name, email (if auth added)
- Start date
- Total learning time
- Preferences (editor theme, font size, etc.)

**Progress Data**
- Lessons completed (boolean per lesson)
- Quizzes attempted (scores, timestamps)
- Exercises completed (boolean, attempts count)
- Checkpoints passed (scores)
- Current location (week, lesson)
- Streak data (days active)

**User-Generated Content**
- Code for each exercise (current + historical snapshots)
- Quiz answers (for review)
- Playground configs saved
- Bookmarks / notes (future feature)

**Analytics/Metadata**
- Time spent per lesson
- Hints used count
- Solution views count
- Retake counts

### Storage Strategy

**Phase 1 (MVP): LocalStorage**
- Pros: Simple, no backend needed, works offline
- Cons: Limited to 5-10MB, not synced across devices
- Use for: All user data, code, progress
- Structure: JSON objects keyed by lesson ID

**Phase 2: Database (PostgreSQL)**
- User account table
- Progress table (user_id, lesson_id, status, score, timestamp)
- Code submissions table (user_id, exercise_id, code, timestamp)
- Sync to cloud, accessible across devices

**Hybrid Approach:**
- LocalStorage for immediate auto-save
- Periodic sync to backend (every 5 minutes if online)
- "Save to cloud" button for manual sync

---

## Technical Implementation Notes

### Code Execution Architecture

**Option 1: Pyodide (Browser-based)**
```
User writes code in Monaco Editor
  â†“
Code sent to Web Worker (Pyodide)
  â†“
Pyodide executes Python in WebAssembly
  â†“
stdout, plots, errors returned to main thread
  â†“
Displayed in Output Panel
```

**Pros:**
- No server needed
- Instant execution
- Works offline
- Sandboxed (secure)

**Cons:**
- Slower than native Python
- Package limitations (no PyTorch initially)
- 5-10s initial load time
- Memory constraints

**Good for:** Weeks 1-4 (numpy, basic RL)

**Option 2: Server Execution (Backend API)**
```
User writes code in Monaco Editor
  â†“
Code sent to FastAPI backend
  â†“
Backend spins up Docker container
  â†“
Executes code in isolated environment
  â†“
Returns stdout, plots (base64), errors
  â†“
Frontend displays results
```

**Pros:**
- Full package support (PyTorch, etc.)
- Faster execution
- GPU access possible

**Cons:**
- Server costs
- Security complexity (code injection)
- Latency (network round-trip)
- Requires backend

**Good for:** Weeks 5+ (neural networks)

### Recommended Hybrid
- Weeks 1-4: Pyodide
- Weeks 5+: Server execution
- User doesn't notice the switch (same UI)

---

## Content Structure

### Curriculum Data Format (JSON)

```json
{
  "weeks": [
    {
      "id": "week-1",
      "number": 1,
      "title": "RL Foundations",
      "description": "...",
      "estimatedHours": 8,
      "lessons": [
        {
          "id": "week-1-lesson-1",
          "type": "theory",
          "title": "Introduction to RL",
          "content": "path/to/markdown/file.md",
          "estimatedMinutes": 30
        },
        {
          "id": "week-1-quiz-1",
          "type": "quiz",
          "title": "RL Basics Quiz",
          "questions": [
            {
              "id": "q1",
              "type": "multiple-choice",
              "question": "What is a policy?",
              "options": ["...", "...", "..."],
              "correctAnswer": 1,
              "explanation": "...",
              "hints": ["...", "..."]
            }
          ]
        },
        {
          "id": "week-1-exercise-1",
          "type": "exercise",
          "title": "Implement Multi-Armed Bandit",
          "instructions": "path/to/instructions.md",
          "starterCode": "path/to/starter.py",
          "solution": "path/to/solution.py",
          "testCases": "path/to/tests.py",
          "hints": ["...", "..."]
        },
        {
          "id": "week-1-playground",
          "type": "playground",
          "title": "Bandit Playground",
          "environment": "multi_armed_bandit",
          "algorithms": ["epsilon-greedy", "ucb", "thompson"],
          "defaultParams": {...}
        }
      ]
    }
  ],
  "checkpoints": [
    {
      "id": "checkpoint-week-1",
      "weekId": "week-1",
      "questions": [...],
      "passThreshold": 0.7
    }
  ]
}
```

### File Organization
```
/curriculum-content/
  /week-01/
    /lessons/
      lesson-1-intro.md
      lesson-2-mdp.md
    /exercises/
      exercise-1-bandit/
        instructions.md
        starter.py
        solution.py
        tests.py
    /assets/
      bellman-diagram.png
  /week-02/
    ...
  curriculum.json (master index)
```

---

## Success Metrics & Analytics

### User Engagement
- Daily active users
- Average session duration
- Lessons completed per week
- Completion rate (% who finish Week 1, Week 2, etc.)
- Drop-off points (where users stop)

### Learning Effectiveness
- Quiz scores by topic
- Exercise completion rate
- Time to complete exercises
- Hints used frequency
- Solution views frequency
- Checkpoint pass rates

### Feature Usage
- Code runs per exercise (iteration count)
- Playground usage rate
- Comparison feature usage
- Reference section visits

### Quality Indicators
- User feedback (thumbs up/down)
- Bug reports
- Feature requests
- Time spent debugging (high = bad UX)

---

## Future Enhancements (Post-MVP)

### Phase 2 Features
- User accounts & authentication
- Cloud sync across devices
- Social features (leaderboards, share progress)
- Discussion forums per lesson
- Peer code review

### Phase 3 Features
- Video explanations embedded in lessons
- AI teaching assistant (chatbot for questions)
- Custom learning paths (skip/focus topics)
- Certificate generation
- Project showcase gallery

### Advanced Features
- Live coding challenges (timed)
- Collaborative coding (pair programming)
- Code quality analysis
- Performance profiling for RL code
- Integration with Jupyter notebooks (export)

---

## Open Questions

1. **Authentication:** Start without it (localStorage) or require from day 1?
   - **Recommendation:** No auth for MVP, add in Phase 2

2. **Content licensing:** What license for curriculum content?
   - **Recommendation:** MIT or CC BY-SA for open sharing

3. **Monetization:** Free forever or freemium model?
   - **Recommendation:** Free for Weeks 1-4, paid for 5-16? Or fully free, optional "Pro" features?

4. **Community:** Built-in forums or link to Discord/Reddit?
   - **Recommendation:** Start with external community, build-in later

5. **Code sandboxing:** How strict? Allow file I/O, network calls?
   - **Recommendation:** Weeks 1-4 no restrictions (Pyodide), 5+ strict sandbox

---

## MVP Scope Definition

### Must-Have (Phase 1 - 3 weeks)
âœ… Dashboard with progress tracking
âœ… Theory lessons (markdown rendering)
âœ… Quizzes with instant feedback
âœ… Coding exercises with Pyodide execution
âœ… Output display (console + plots)
âœ… Test cases with pass/fail
âœ… LocalStorage for progress
âœ… Weeks 1-4 content (tabular methods)
âœ… Basic responsive design

### Nice-to-Have (Phase 1.5)
âœ… Playground for Week 1-2
âœ… Hints system
âœ… Solutions (time-locked)
âœ… Reference/cheatsheet
âœ… Checkpoint quizzes

### Phase 2 (Weeks 4-8)
- User authentication
- Backend API (FastAPI)
- Database persistence
- Server-side code execution
- Weeks 5-16 content
- Advanced visualizations

---

## Conclusion

This webapp is absolutely feasible and would be an excellent portfolio project. The MVP can be built in 2-3 weeks focusing on Weeks 1-4 of the curriculum with browser-based code execution.

**Key Success Factors:**
1. Start small (MVP = Weeks 1-4)
2. Use Pyodide to avoid backend complexity initially
3. Focus on core learning loop (read â†’ quiz â†’ code â†’ feedback)
4. Excellent UX for code editor and visualizations
5. Clear progress indicators for motivation

**Risks to Mitigate:**
- Scope creep (stick to MVP!)
- Code execution security (use Pyodide first)
- Content creation time (curriculum writing takes longer than coding)
- Browser compatibility (test on multiple browsers)

**Next Steps:**
1. Validate tech stack (build tiny Pyodide + Monaco POC)
2. Design database schema (even if using localStorage first)
3. Create 1-2 lessons of content to test structure
4. Build coding exercise template/framework
5. Iterate on UX with real lessons

Would you like to dive deeper into any specific aspect (tech stack validation, content structure, specific screen design)?